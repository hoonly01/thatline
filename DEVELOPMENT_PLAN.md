# ThatLine iOS On-Device OCR ì „í™˜ ê°œë°œ ê³„íšì„œ

**ì‘ì„±ì¼**: 2025-11-09
**í”„ë¡œì íŠ¸**: ThatLine (ê·¸ë•Œ ê·¸ ë¬¸ì¥)
**ëª©í‘œ**: Google Cloud Vision API â†’ Apple Vision Framework ì „í™˜
**ë°©ì‹**: Flutter + iOS Vision Framework (Method Channel)
**ì˜ˆìƒ ê¸°ê°„**: 2-3ì¼

---

## ğŸ“‹ Executive Summary

### í˜„ì¬ ìƒí™©
- Google Cloud Vision API ì‚¬ìš© ì¤‘ (ì„œë²„ ê¸°ë°˜ OCR)
- ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„± + API ë¹„ìš© ë°œìƒ
- ì˜¤í”„ë¼ì¸ ì‚¬ìš© ë¶ˆê°€ëŠ¥

### ëª©í‘œ
- On-device AIë¡œ ì „í™˜ (Apple Vision Framework)
- ì˜¤í”„ë¼ì¸ OCR ì§€ì›
- API ë¹„ìš© ì œê±°
- ì‘ë‹µ ì†ë„ ê°œì„  (2-3ì´ˆ â†’ 0.5-1ì´ˆ)

### ì „ëµ
- Flutter UI 100% ìœ ì§€
- iOS Vision Frameworkë¥¼ Method Channelë¡œ ì—°ê²°
- ê¸°ì¡´ ì„œë²„ OCR ì—”ë“œí¬ì¸íŠ¸ëŠ” fallbackìœ¼ë¡œ ë³´ì¡´

---

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ (SMART)

| í•­ëª© | ëª©í‘œ |
|------|------|
| **Specific** | iOSì—ì„œ Apple Vision Framework ê¸°ë°˜ On-device OCR êµ¬í˜„ |
| **Measurable** | í•œêµ­ì–´ ì¸ì‹ë¥  95%+, ì²˜ë¦¬ ì†ë„ 1ì´ˆ ì´ë‚´ |
| **Achievable** | ê¸°ì¡´ Flutter ì½”ë“œ ìœ ì§€, Method Channel ì¶”ê°€ë§Œìœ¼ë¡œ ê°€ëŠ¥ |
| **Relevant** | ì˜¤í”„ë¼ì¸ ì§€ì› + ë¹„ìš© ì ˆê° + ì‚¬ìš©ì ê²½í—˜ ê°œì„  |
| **Time-bound** | 2-3ì¼ ë‚´ ì™„ë£Œ |

---

## ğŸ“Š í˜„ì¬ í”„ë¡œì íŠ¸ ë¶„ì„

### ì½”ë“œë² ì´ìŠ¤ í˜„í™©
```
Client (Flutter):
â”œâ”€â”€ lib/screens/camera_screen.dart      (189ì¤„) â† OCR í˜¸ì¶œ ë¶€ë¶„
â”œâ”€â”€ lib/services/api_service.dart       (115ì¤„) â† OCR API í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ lib/models/ocr_result.dart          (57ì¤„)  â† ë°ì´í„° ëª¨ë¸
â””â”€â”€ ê¸°íƒ€ UI íŒŒì¼ë“¤ (ì•½ 900ì¤„)

Server (Dart):
â””â”€â”€ lib/handlers/ocr_handler.dart       (191ì¤„) â† Google Vision API í˜¸ì¶œ

ì´ ë¼ì¸ ìˆ˜: ~1,244ì¤„
```

### í˜„ì¬ OCR í”Œë¡œìš°
```
1. ì‚¬ìš©ìê°€ ê°¤ëŸ¬ë¦¬ì—ì„œ ì´ë¯¸ì§€ ì„ íƒ
2. camera_screen.dart â†’ ì„œë²„ë¡œ ì´ë¯¸ì§€ ì—…ë¡œë“œ
3. ì„œë²„ â†’ Google Cloud Vision API í˜¸ì¶œ
4. ì„œë²„ â†’ OCR ê²°ê³¼ ë°˜í™˜
5. camera_screen.dart â†’ ë”ë¯¸ ë°ì´í„°ë¡œ ë¬¸ì¥ ì €ì¥ (!)

í˜„ì¬ ë¬¸ì œ: ì‹¤ì œ OCR ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
```

### ë³€ê²½ ëŒ€ìƒ íŒŒì¼
```
ìˆ˜ì •:
  âœï¸  ios/Runner/AppDelegate.swift         (Vision Framework ì¶”ê°€)
  âœï¸  lib/services/api_service.dart         (ìƒˆ ë©”ì„œë“œ ì¶”ê°€)
  âœï¸  lib/screens/camera_screen.dart        (OCR í˜¸ì¶œ ë¡œì§ ë³€ê²½)

ì‹ ê·œ:
  â• lib/services/on_device_ocr_service.dart (Method Channel ë˜í¼)
  â• ios/Runner/VisionOCRHandler.swift       (Vision Framework êµ¬í˜„)

ì„ íƒì :
  ğŸ—‘ï¸  server/lib/handlers/ocr_handler.dart   (ì œê±° or Fallback ìœ ì§€)
```

---

## ğŸ—“ï¸ ì„¸ë¶€ ê°œë°œ ì¼ì •

### **Day 1: iOS Native êµ¬í˜„** (6-8ì‹œê°„)

#### Phase 1.1: iOS Vision Framework í•¸ë“¤ëŸ¬ êµ¬í˜„ (3ì‹œê°„)
- [ ] VisionOCRHandler.swift íŒŒì¼ ìƒì„±
- [ ] VNRecognizeTextRequest êµ¬í˜„
- [ ] í•œêµ­ì–´/ì˜ì–´ ì¸ì‹ ì„¤ì •
- [ ] Bounding Box ì •ë³´ ì¶”ì¶œ (ì„ íƒ)
- [ ] ì—ëŸ¬ ì²˜ë¦¬

**ì˜ˆìƒ ì½”ë“œ:**
```swift
// ios/Runner/VisionOCRHandler.swift
import Vision
import UIKit

class VisionOCRHandler {
    func recognizeText(
        imagePath: String,
        completion: @escaping (Result<[String], Error>) -> Void
    ) {
        guard let image = UIImage(contentsOfFile: imagePath),
              let cgImage = image.cgImage else {
            completion(.failure(OCRError.invalidImage))
            return
        }

        let request = VNRecognizeTextRequest { request, error in
            if let error = error {
                completion(.failure(error))
                return
            }

            guard let observations = request.results as? [VNRecognizedTextObservation]
            else {
                completion(.success([]))
                return
            }

            let texts = observations.compactMap { observation in
                observation.topCandidates(1).first?.string
            }

            completion(.success(texts))
        }

        // ê³ ì •ë°€ ì¸ì‹ ëª¨ë“œ
        request.recognitionLevel = .accurate

        // í•œêµ­ì–´ + ì˜ì–´ ìš°ì„ 
        request.recognitionLanguages = ["ko-KR", "en-US"]

        // ì–¸ì–´ êµì • í™œì„±í™”
        request.usesLanguageCorrection = true

        // ìµœì†Œ í…ìŠ¤íŠ¸ ë†’ì´ (ì‘ì€ ê¸€ì”¨ ë¬´ì‹œ)
        request.minimumTextHeight = 0.02

        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

        do {
            try handler.perform([request])
        } catch {
            completion(.failure(error))
        }
    }
}
```

#### Phase 1.2: Method Channel ì—°ê²° (2ì‹œê°„)
- [ ] AppDelegate.swift ìˆ˜ì •
- [ ] FlutterMethodChannel ë“±ë¡
- [ ] VisionOCRHandler í†µí•©
- [ ] Flutter ê²°ê³¼ ë°˜í™˜ í¬ë§· ì •ì˜

**ì˜ˆìƒ ì½”ë“œ:**
```swift
// ios/Runner/AppDelegate.swift
import Flutter
import UIKit

@main
@objc class AppDelegate: FlutterAppDelegate {
    private let ocrHandler = VisionOCRHandler()

    override func application(
        _ application: UIApplication,
        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?
    ) -> Bool {
        setupMethodChannels()
        return super.application(application, didFinishLaunchingWithOptions: launchOptions)
    }

    private func setupMethodChannels() {
        guard let controller = window?.rootViewController as? FlutterViewController
        else { return }

        let ocrChannel = FlutterMethodChannel(
            name: "com.thatline/ocr",
            binaryMessenger: controller.binaryMessenger
        )

        ocrChannel.setMethodCallHandler { [weak self] call, result in
            guard let self = self else { return }

            switch call.method {
            case "recognizeText":
                self.handleRecognizeText(call: call, result: result)
            default:
                result(FlutterMethodNotImplemented)
            }
        }
    }

    private func handleRecognizeText(call: FlutterMethodCall, result: @escaping FlutterResult) {
        guard let args = call.arguments as? [String: Any],
              let imagePath = args["imagePath"] as? String
        else {
            result(FlutterError(code: "INVALID_ARGS", message: "Missing imagePath", details: nil))
            return
        }

        ocrHandler.recognizeText(imagePath: imagePath) { ocrResult in
            switch ocrResult {
            case .success(let texts):
                result(texts)
            case .failure(let error):
                result(FlutterError(
                    code: "OCR_ERROR",
                    message: error.localizedDescription,
                    details: nil
                ))
            }
        }
    }
}
```

#### Phase 1.3: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (iOS) (1ì‹œê°„)
- [ ] Xcodeì—ì„œ Swift ì½”ë“œ ì»´íŒŒì¼ í™•ì¸
- [ ] ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ OCR í…ŒìŠ¤íŠ¸
- [ ] í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ì¦

---

### **Day 2: Flutter í†µí•©** (6-8ì‹œê°„)

#### Phase 2.1: Flutter OCR Service êµ¬í˜„ (2ì‹œê°„)
- [ ] OnDeviceOCRService í´ë˜ìŠ¤ ìƒì„±
- [ ] Method Channel ë˜í¼ êµ¬í˜„
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì˜ˆì™¸ í•¸ë“¤ë§
- [ ] í”Œë«í¼ ê°ì§€ (iOSë§Œ ì§€ì›)

**ì˜ˆìƒ ì½”ë“œ:**
```dart
// lib/services/on_device_ocr_service.dart
import 'dart:io';
import 'package:flutter/services.dart';

class OnDeviceOCRService {
  static const _channel = MethodChannel('com.thatline/ocr');

  /// ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹
  ///
  /// [imagePath]: ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
  /// Returns: ì¸ì‹ëœ í…ìŠ¤íŠ¸ ëª©ë¡ (ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì”©)
  Future<List<String>> recognizeText(String imagePath) async {
    if (!Platform.isIOS) {
      throw UnsupportedError('On-device OCR is only supported on iOS');
    }

    try {
      final result = await _channel.invokeMethod('recognizeText', {
        'imagePath': imagePath,
      });

      if (result == null) {
        return [];
      }

      return List<String>.from(result);
    } on PlatformException catch (e) {
      throw OCRException(
        'Vision Framework error: ${e.message}',
        code: e.code,
        details: e.details,
      );
    } catch (e) {
      throw OCRException('Unexpected OCR error: $e');
    }
  }

  /// ì—¬ëŸ¬ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
  String joinRecognizedTexts(List<String> texts, {String separator = ' '}) {
    return texts.join(separator).trim();
  }
}

class OCRException implements Exception {
  final String message;
  final String? code;
  final dynamic details;

  OCRException(this.message, {this.code, this.details});

  @override
  String toString() => 'OCRException: $message';
}
```

#### Phase 2.2: Camera Screen ìˆ˜ì • (3ì‹œê°„)
- [ ] OnDeviceOCRService í†µí•©
- [ ] _uploadImage ë©”ì„œë“œ ë¦¬íŒ©í† ë§
- [ ] ì‹¤ì œ OCR ê²°ê³¼ë¡œ ë¬¸ì¥ ì €ì¥ (ë”ë¯¸ ë°ì´í„° ì œê±°)
- [ ] ë¡œë”© ìƒíƒœ ê°œì„ 
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 

**ì˜ˆìƒ ì½”ë“œ:**
```dart
// lib/screens/camera_screen.dart (ìˆ˜ì •)
import 'package:thatline_client/services/on_device_ocr_service.dart';

class _CameraScreenState extends State<CameraScreen> {
  final _ocrService = OnDeviceOCRService();

  Future<void> _uploadImage(File imageFile) async {
    if (!mounted) return;
    setState(() => _isLoading = true);

    try {
      // 1ï¸âƒ£ On-device OCR ì‹¤í–‰
      final recognizedTexts = await _ocrService.recognizeText(imageFile.path);

      if (recognizedTexts.isEmpty) {
        throw Exception('ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      }

      // 2ï¸âƒ£ ì²« ë²ˆì§¸ ì¤„ì„ ë¬¸ì¥ìœ¼ë¡œ ì‚¬ìš© (ë˜ëŠ” ì „ì²´ í…ìŠ¤íŠ¸)
      final extractedText = recognizedTexts.first;

      // 3ï¸âƒ£ ë¬¸ì¥ í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸ í‘œì‹œ (ì‚¬ìš©ìê°€ ìˆ˜ì • ê°€ëŠ¥)
      final editedText = await _showEditDialog(extractedText);

      if (editedText == null || editedText.isEmpty) {
        // ì‚¬ìš©ìê°€ ì·¨ì†Œí•¨
        return;
      }

      // 4ï¸âƒ£ ì„œë²„ì— ë¬¸ì¥ ì €ì¥
      final sentenceData = {
        'sentenceId': 'ocr-${DateTime.now().millisecondsSinceEpoch}',
        'text': editedText,
        'bookName': 'ìŠ¤ìº”í•œ ì±…',  // ë‚˜ì¤‘ì— ì…ë ¥ë°›ë„ë¡ ê°œì„  ê°€ëŠ¥
        'bookWriter': 'ì‘ê°€ ë¯¸ìƒ',
        'date': DateTime.now().toIso8601String(),
        'imageUrl': '',
      };

      final response = await http.post(
        Uri.parse('http://localhost:8080/sentences'),
        headers: {'Content-Type': 'application/json'},
        body: jsonEncode(sentenceData),
      );

      if (response.statusCode == 200 || response.statusCode == 201) {
        _showSuccessDialog('ë¬¸ì¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: "$editedText"');
      } else {
        throw Exception('ë¬¸ì¥ ì €ì¥ ì‹¤íŒ¨: ${response.statusCode}');
      }
    } on OCRException catch (e) {
      _showErrorDialog('í…ìŠ¤íŠ¸ ì¸ì‹ ì‹¤íŒ¨: ${e.message}');
    } catch (e) {
      _showErrorDialog('ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: $e');
    } finally {
      if (mounted) setState(() => _isLoading = false);
    }
  }

  /// OCR ê²°ê³¼ë¥¼ ì‚¬ìš©ìê°€ í¸ì§‘í•  ìˆ˜ ìˆëŠ” ë‹¤ì´ì–¼ë¡œê·¸
  Future<String?> _showEditDialog(String initialText) async {
    final controller = TextEditingController(text: initialText);

    return showDialog<String>(
      context: context,
      builder: (context) => AlertDialog(
        title: const Text('ì¸ì‹ëœ ë¬¸ì¥'),
        content: TextField(
          controller: controller,
          maxLines: 3,
          decoration: const InputDecoration(
            hintText: 'í•„ìš”ì‹œ ìˆ˜ì •í•˜ì„¸ìš”',
          ),
        ),
        actions: [
          TextButton(
            onPressed: () => Navigator.pop(context, null),
            child: const Text('ì·¨ì†Œ'),
          ),
          TextButton(
            onPressed: () => Navigator.pop(context, controller.text),
            child: const Text('ì €ì¥'),
          ),
        ],
      ),
    );
  }
}
```

#### Phase 2.3: API Service ì—…ë°ì´íŠ¸ (1ì‹œê°„)
- [ ] extractText ë©”ì„œë“œ deprecated ë§ˆí‚¹
- [ ] ì£¼ì„ ì—…ë°ì´íŠ¸
- [ ] Fallback ë¡œì§ ì¶”ê°€ (ì„ íƒ)

---

### **Day 3: í…ŒìŠ¤íŠ¸ ë° ìµœì í™”** (4-6ì‹œê°„)

#### Phase 3.1: í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
- [ ] iOS Simulatorì—ì„œ ì•± ì‹¤í–‰
- [ ] ê°¤ëŸ¬ë¦¬ì—ì„œ ì´ë¯¸ì§€ ì„ íƒ â†’ OCR í…ŒìŠ¤íŠ¸
- [ ] í•œêµ­ì–´ ì±… ì‚¬ì§„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
- [ ] ì˜ì–´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ (ë¹ˆ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì—†ìŒ ë“±)

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**
```
âœ… TC-001: í•œêµ­ì–´ ì±… í‘œì§€ ì¸ì‹
   ì…ë ¥: í•œê°• ì‘ê°€ì˜ "ì±„ì‹ì£¼ì˜ì" í‘œì§€
   ì˜ˆìƒ: "ì±„ì‹ì£¼ì˜ì", "í•œê°•" ì¸ì‹

âœ… TC-002: í•œêµ­ì–´ ë³¸ë¬¸ ì¸ì‹
   ì…ë ¥: ì±… ë³¸ë¬¸ ì‚¬ì§„
   ì˜ˆìƒ: ë¬¸ì¥ ì •í™•íˆ ì¶”ì¶œ

âœ… TC-003: ì˜ì–´ í…ìŠ¤íŠ¸ ì¸ì‹
   ì…ë ¥: ì˜ì–´ ì±… ì‚¬ì§„
   ì˜ˆìƒ: ì˜ì–´ ë¬¸ì¥ ì¸ì‹

âœ… TC-004: í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€
   ì…ë ¥: í’ê²½ ì‚¬ì§„
   ì˜ˆìƒ: "í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì—ëŸ¬ ë©”ì‹œì§€

âœ… TC-005: ì €í™”ì§ˆ ì´ë¯¸ì§€
   ì…ë ¥: íë¦¿í•œ ì‚¬ì§„
   ì˜ˆìƒ: ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì¸ì‹ or ì ì ˆí•œ ì—ëŸ¬
```

#### Phase 3.2: UI/UX ê°œì„  (2ì‹œê°„)
- [ ] ë¡œë”© ì¸ë””ì¼€ì´í„° ê°œì„ 
- [ ] OCR ì§„í–‰ ìƒíƒœ í‘œì‹œ ("í…ìŠ¤íŠ¸ ì¸ì‹ ì¤‘...")
- [ ] ì„±ê³µ ë©”ì‹œì§€ ê°œì„ 
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ìˆ˜ì •

**ê°œì„  ì‚¬í•­:**
```dart
// ë¡œë”© ìƒíƒœ ê°œì„ 
Widget _buildLoadingOverlay() {
  return Container(
    color: Colors.black54,
    child: Center(
      child: Column(
        mainAxisSize: MainAxisSize.min,
        children: [
          const CircularProgressIndicator(color: Colors.white),
          const SizedBox(height: 16),
          Text(
            'ì±… ì† ë¬¸ì¥ì„ ì½ê³  ìˆì–´ìš”...',
            style: TextStyle(color: Colors.white, fontSize: 16),
          ),
        ],
      ),
    ),
  );
}
```

#### Phase 3.3: ì„±ëŠ¥ ìµœì í™” (1ì‹œê°„)
- [ ] ì´ë¯¸ì§€ ì••ì¶• (í•„ìš”ì‹œ)
- [ ] OCR ê²°ê³¼ ìºì‹± (ì„ íƒ)
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸

---

### **Day 4: ì„œë²„ ì •ë¦¬ ë° ë°°í¬ ì¤€ë¹„** (ì„ íƒ, 2-3ì‹œê°„)

#### Phase 4.1: ì„œë²„ ì •ë¦¬ (1ì‹œê°„)
- [ ] OCR ì—”ë“œí¬ì¸íŠ¸ ì œê±° OR
- [ ] Fallbackìœ¼ë¡œ ìœ ì§€ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)
- [ ] ë¬¸ì„œí™” ì—…ë°ì´íŠ¸

**ì„ íƒì§€:**
```dart
// Option 1: ì™„ì „ ì œê±°
// server/lib/handlers/ocr_handler.dart ì‚­ì œ
// server/bin/server.dartì—ì„œ ë¼ìš°íŠ¸ ì œê±°

// Option 2: Fallback ìœ ì§€
Future<List<String>> recognizeText(String imagePath) async {
  try {
    // 1ìˆœìœ„: On-device OCR
    return await _ocrService.recognizeText(imagePath);
  } catch (e) {
    // 2ìˆœìœ„: ì„œë²„ OCR (ë„¤íŠ¸ì›Œí¬ í•„ìš”)
    return await _apiService.extractText(imageBytes);
  }
}
```

#### Phase 4.2: ë¹Œë“œ ìµœì í™” (1ì‹œê°„)
- [ ] flutter build ios --release ì‹¤í–‰
- [ ] ì•± í¬ê¸° í™•ì¸ (< 50MB ëª©í‘œ)
- [ ] ë¶ˆí•„ìš”í•œ ì˜ì¡´ì„± ì œê±°
- [ ] Assets ìµœì í™”

```bash
# ë¹Œë“œ í¬ê¸° ë¶„ì„
flutter build ios --release --analyze-size

# ì˜ˆìƒ ê²°ê³¼:
# Flutter Engine: 35MB
# Dart AOT: 3MB
# Assets: 5-10MB
# Total: 43-48MB âœ…
```

#### Phase 4.3: App Store ì¤€ë¹„ (1ì‹œê°„)
- [ ] Application ID ë³€ê²½ (com.example.thatline_client â†’ ì‹¤ì œ ID)
- [ ] ë¦´ë¦¬ì¦ˆ ì„œëª… ì„¤ì •
- [ ] Privacy Manifest ì¶”ê°€ (iOS 17+)
- [ ] App Store Connect ì •ë³´ ì¤€ë¹„

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### Unit Tests
```dart
// test/services/on_device_ocr_service_test.dart
void main() {
  group('OnDeviceOCRService', () {
    test('should throw UnsupportedError on non-iOS platform', () {
      // Test implementation
    });

    test('should handle empty recognition results', () {
      // Test implementation
    });
  });
}
```

### Integration Tests
```dart
// integration_test/ocr_flow_test.dart
void main() {
  testWidgets('Complete OCR flow', (tester) async {
    // 1. ì•± ì‹¤í–‰
    // 2. ì¹´ë©”ë¼ í™”ë©´ ì´ë™
    // 3. ì´ë¯¸ì§€ ì„ íƒ
    // 4. OCR ê²°ê³¼ í™•ì¸
    // 5. ë¬¸ì¥ ì €ì¥ í™•ì¸
  });
}
```

### Manual Tests (iOS Simulator)
- [ ] iPhone 15 Pro Simulator
- [ ] iPhone SE Simulator (ì‘ì€ í™”ë©´)
- [ ] iPad Simulator
- [ ] ë‹¤í¬ëª¨ë“œ í…ŒìŠ¤íŠ¸

---

## âš ï¸ ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ ë°©ì•ˆ

### ìœ„í—˜ 1: Vision Framework ì¸ì‹ë¥  ë‚®ìŒ
**í™•ë¥ **: ë‚®ìŒ (10%)
**ì˜í–¥**: ë†’ìŒ
**ëŒ€ì‘**:
- VNRecognizeTextRequest íŒŒë¼ë¯¸í„° íŠœë‹
- `recognitionLevel = .accurate` ì‚¬ìš©
- `customWords` ì¶”ê°€ (ìì£¼ ë‚˜ì˜¤ëŠ” ì±… ì œëª©/ì‘ê°€ëª…)
- Fallbackìœ¼ë¡œ Google Vision API ìœ ì§€

### ìœ„í—˜ 2: Method Channel í†µì‹  ì˜¤ë¥˜
**í™•ë¥ **: ì¤‘ê°„ (30%)
**ì˜í–¥**: ë†’ìŒ
**ëŒ€ì‘**:
- ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
- Try-catchë¡œ ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬
- ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€
- ì¬ì‹œë„ ë¡œì§ ì¶”ê°€

### ìœ„í—˜ 3: ì´ë¯¸ì§€ í˜•ì‹ í˜¸í™˜ì„± ë¬¸ì œ
**í™•ë¥ **: ë‚®ìŒ (15%)
**ì˜í–¥**: ì¤‘ê°„
**ëŒ€ì‘**:
- UIImage â†’ CGImage ë³€í™˜ ì‹¤íŒ¨ ì²˜ë¦¬
- ì§€ì› í˜•ì‹ ëª…ì‹œ (JPG, PNG, HEIC)
- ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (íšŒì „, í¬ê¸° ì¡°ì •)

### ìœ„í—˜ 4: iOS ë²„ì „ í˜¸í™˜ì„±
**í™•ë¥ **: ë‚®ìŒ (10%)
**ì˜í–¥**: ì¤‘ê°„
**ëŒ€ì‘**:
- iOS 13+ íƒ€ê²Ÿ (Vision Framework ì•ˆì •í™”)
- Deployment Target í™•ì¸
- êµ¬ë²„ì „ iOSì—ì„œ ê¸°ëŠ¥ ì œí•œ ì•ˆë‚´

---

## ğŸ“¦ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ í’ˆì§ˆ
- [ ] ëª¨ë“  TODO ì£¼ì„ í•´ê²°
- [ ] Dead code ì œê±°
- [ ] í•˜ë“œì½”ë”©ëœ URL í™˜ê²½ë³€ìˆ˜í™”
- [ ] ë¡œê·¸ ë ˆë²¨ ì¡°ì • (DEBUG â†’ INFO)

### ë³´ì•ˆ
- [ ] API í‚¤ ë…¸ì¶œ í™•ì¸ (config.json ì œì™¸)
- [ ] .gitignore ì—…ë°ì´íŠ¸
- [ ] Privacy Manifest ì¶”ê°€
- [ ] Camera/Photo Library ê¶Œí•œ ì„¤ëª… ì¶”ê°€

### ì„±ëŠ¥
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
- [ ] ì•± ì‹œì‘ ì‹œê°„ ì¸¡ì • (< 2ì´ˆ)
- [ ] OCR ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • (< 1.5ì´ˆ)

### ë¬¸ì„œí™”
- [ ] README.md ì—…ë°ì´íŠ¸
- [ ] CHANGELOG.md ì‘ì„±
- [ ] API ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì¶”ê°€

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ (KPI)

| ì§€í‘œ | í˜„ì¬ (Cloud) | ëª©í‘œ (On-device) |
|------|--------------|------------------|
| **OCR ì²˜ë¦¬ ì‹œê°„** | 2-3ì´ˆ | < 1ì´ˆ |
| **ì˜¤í”„ë¼ì¸ ì§€ì›** | âŒ | âœ… |
| **ì›”ê°„ API ë¹„ìš©** | $X | $0 |
| **ì•± í¬ê¸°** | 43MB | < 50MB |
| **í•œêµ­ì–´ ì¸ì‹ë¥ ** | 98% | > 95% |
| **ì‚¬ìš©ì ë§Œì¡±ë„** | - | 4.5+/5.0 |

---

## ğŸ¯ ë§ˆì¼ìŠ¤í†¤

```
âœ… M1: ê°œë°œ ê³„íš ìˆ˜ë¦½                    [Day 0]
â¬œ M2: iOS Vision Framework êµ¬í˜„         [Day 1]
â¬œ M3: Flutter í†µí•© ì™„ë£Œ                [Day 2]
â¬œ M4: í…ŒìŠ¤íŠ¸ í†µê³¼                      [Day 3]
â¬œ M5: App Store ì œì¶œ ì¤€ë¹„              [Day 4]
```

---

## ğŸ“ ì˜ì‚¬ê²°ì • í¬ì¸íŠ¸

### Decision Point 1: ì„œë²„ OCR ì²˜ë¦¬ (Day 2)
**ì§ˆë¬¸**: ê¸°ì¡´ ì„œë²„ OCR ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€?
**ì˜µì…˜**:
- A) ì™„ì „ ì œê±° (ê¶Œì¥)
- B) Fallbackìœ¼ë¡œ ìœ ì§€
- C) ì‚¬ìš©ì ì„¤ì •ìœ¼ë¡œ ì„ íƒ ê°€ëŠ¥

**ê¶Œì¥**: B) Fallback ìœ ì§€ (ì•ˆì „ì„±)

### Decision Point 2: OCR ê²°ê³¼ í¸ì§‘ (Day 2)
**ì§ˆë¬¸**: ì‚¬ìš©ìê°€ OCR ê²°ê³¼ë¥¼ í¸ì§‘í•  ìˆ˜ ìˆì–´ì•¼ í•˜ëŠ”ê°€?
**ì˜µì…˜**:
- A) ìë™ ì €ì¥ (í¸ì§‘ ë¶ˆê°€)
- B) í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸ í‘œì‹œ (ê¶Œì¥)
- C) ì „ìš© í¸ì§‘ í™”ë©´

**ê¶Œì¥**: B) í¸ì§‘ ë‹¤ì´ì–¼ë¡œê·¸

---

## ğŸ“š ì°¸ê³  ìë£Œ

### Apple ê³µì‹ ë¬¸ì„œ
- [Vision Framework Documentation](https://developer.apple.com/documentation/vision)
- [VNRecognizeTextRequest](https://developer.apple.com/documentation/vision/vnrecognizetextrequest)
- [Text Recognition Best Practices](https://developer.apple.com/documentation/vision/recognizing_text_in_images)

### Flutter ë¬¸ì„œ
- [Platform Channels](https://docs.flutter.dev/development/platform-integration/platform-channels)
- [Method Channel](https://api.flutter.dev/flutter/services/MethodChannel-class.html)

### ì½”ë“œ ì˜ˆì œ
- [flutter/samples - platform_channels](https://github.com/flutter/samples/tree/main/platform_channels)

---

## ğŸš€ ì‹œì‘ ì¤€ë¹„

### ê°œë°œ í™˜ê²½ í™•ì¸
```bash
# Flutter ë²„ì „
flutter --version
# Flutter 3.2.3+ í•„ìš”

# iOS ê°œë°œ í™˜ê²½
xcodebuild -version
# Xcode 15+ ê¶Œì¥

# CocoaPods
pod --version

# iOS Simulator í™•ì¸
xcrun simctl list devices
```

### í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
```bash
cd /home/user/thatline/client

# ì˜ì¡´ì„± ì„¤ì¹˜
flutter pub get

# iOS ì˜ì¡´ì„±
cd ios && pod install && cd ..

# ë¹Œë“œ í…ŒìŠ¤íŠ¸
flutter build ios --debug
```

---

## âœ… ë‹¤ìŒ ë‹¨ê³„

1. **ì´ ê°œë°œ ê³„íšì„œ ìŠ¹ì¸ ë°›ê¸°**
2. **Day 1 ì‘ì—… ì‹œì‘**: iOS Vision Framework êµ¬í˜„
3. **ì¼ì¼ ì§„í–‰ ìƒí™© ë³´ê³ **
4. **ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê³µìœ **

---

**ì‘ì„±ì**: Claude Code
**ê²€í†  í•„ìš”**: í”„ë¡œì íŠ¸ ë¦¬ë”
**ìƒíƒœ**: ìŠ¹ì¸ ëŒ€ê¸° ì¤‘
